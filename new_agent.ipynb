{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from spark_env.env import Environment\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import dgl\n",
    "import torch\n",
    "from spark_env.job_dag import JobDAG\n",
    "from spark_env.node import Node\n",
    "import torch.nn as nn\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import torch.nn.functional as F \n",
    "from numpy.random import randint\n",
    "\n",
    "cuda = \"cuda\"\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, features=5, hidden_layer_size=10, embedding_size=10):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GraphConv(in_feats=features, out_feats=hidden_layer_size)\n",
    "        self.conv2 = GraphConv(in_feats=hidden_layer_size, out_feats=embedding_size)\n",
    "        # self.conv3 = GraphConv(hidden_layer_size, embedding_size)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        h = self.conv1(g, h)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.relu(h)\n",
    "        return h\n",
    "\n",
    "gnn = GCN().to(cuda)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class EnvironmentWrapper:\n",
    "    \n",
    "    def __init__(self, view_range=50, reset_prob=5e-7, max_exec=100, env_len = 500, turn = 40) -> None:\n",
    "\n",
    "        # Set pre built environment\n",
    "        self.env = Environment()\n",
    "        \n",
    "        # environment parameters\n",
    "        self.reset_prob = reset_prob\n",
    "        self.max_exec = max_exec\n",
    "\n",
    "        # wrapper parameters\n",
    "        self.range = view_range\n",
    "        self.env_len = env_len\n",
    "        self.offset = 0\n",
    "        self.turn = turn\n",
    "\n",
    "        # create prebuilt environment\n",
    "        self.reset()\n",
    "\n",
    "        self.frontier_nodes = []\n",
    "        self.source_exec = max_exec\n",
    "        self.logits = None\n",
    "    \n",
    "    # reset the environment to a new seed\n",
    "    def reset(self):\n",
    "        seed = np.random.randint(12, 1234567)\n",
    "        self.env.seed(seed)\n",
    "        # self.env.seed(234)\n",
    "        self.env.reset(max_time=np.random.geometric(self.reset_prob))\n",
    "        self.offset = 0\n",
    "        self.logits = False\n",
    "        self.observe()\n",
    "\n",
    "    # observe and decode an observation into usable paramaters for the agent\n",
    "    # this involves embedding the graph nodes into 10 dim vectors\n",
    "    def observe(self, gnn_reset=True):\n",
    "        # get the new observation from the environement\n",
    "        G, frontier_nodes, num_source_exec, action_map, node_inputs = self.env.new_observation()\n",
    "\n",
    "        # reset the frontier nodes and the number of free executors\n",
    "        # and add index of all the leaf nodes\n",
    "        leaf_nodes = []\n",
    "        self.frontier_nodes = []\n",
    "        for node in frontier_nodes:\n",
    "            self.frontier_nodes.append(node)  \n",
    "            leaf_nodes.append(action_map.inverse_map[node])\n",
    "        self.source_exec = num_source_exec\n",
    "\n",
    "        # calculate the logits and filter the required indices\n",
    "        # based on the number of nodes the agent can see\n",
    "        if gnn_reset and not self.logits:\n",
    "            logits = gnn(G.to(cuda), node_inputs.to(cuda))\n",
    "        else:\n",
    "            logits = self.logits\n",
    "            \n",
    "        required_indices = []\n",
    "        for i in range(1, self.range+1):\n",
    "            if len(required_indices) == len(frontier_nodes):\n",
    "                break\n",
    "            index = (self.offset+i)%len(frontier_nodes)\n",
    "            required_indices.append(leaf_nodes[index])\n",
    "\n",
    "        # pad the output with 0 if 50 nodes are not available\n",
    "        # and add the number of source executors remaining to the vector\n",
    "        logits = logits[required_indices]\n",
    "        padding = torch.zeros(self.range-len(logits), 10).to(cuda)\n",
    "        logits = torch.cat([logits, padding])\n",
    "        logits = logits.flatten()\n",
    "        logits = torch.cat([logits, torch.tensor([self.source_exec]).to(cuda)])\n",
    "\n",
    "        return logits\n",
    "\n",
    "    # perform an action and return the resultant state and reward\n",
    "    def step(self, action, early_stop=True):\n",
    "\n",
    "        # get the direction, job and limit\n",
    "        direction, job, limit = action\n",
    "\n",
    "        # if index is greater than the number of jobs, send a high negative reward\n",
    "        index = (self.offset + job) % self.range\n",
    "        if index >= len(self.frontier_nodes):\n",
    "            index = len(self.frontier_nodes) - 1\n",
    "        if limit > self.source_exec :\n",
    "            limit = self.source_exec\n",
    "        if (len(self.frontier_nodes) > 0 and limit == 0) :\n",
    "            limit = 1\n",
    "        if len(self.frontier_nodes) == 0:\n",
    "            state = self.observe()\n",
    "            reward, done = self.env.new_step(None, self.max_exec)\n",
    "            return state, 0, done\n",
    "\n",
    "        # update the view offset to check for more jobs i.e stay or move right\n",
    "        self.offset += (round(direction)*self.turn)\n",
    "        self.offset = self.offset % self.range\n",
    "\n",
    "        # take a step and observe the reward, completion and the state from the old environement\n",
    "        reward, done = self.env.new_step(self.frontier_nodes[index], limit)\n",
    "        state = self.observe()\n",
    "        \n",
    "        # return None, None, None\n",
    "        return state, reward, done\n",
    "\n",
    "\n",
    "env = EnvironmentWrapper()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from numpy.random import randint\n",
    "for i in range (100):\n",
    "    R = 0\n",
    "    env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    nodes = []\n",
    "    while not done:\n",
    "        action = (randint(2), randint(0, 50), randint(2, 5))\n",
    "        nodes.append(\" source exec : {} - chosen : {} \\n\".format(env.source_exec, action[2]))\n",
    "        state, reward, done = env.step(action)\n",
    "        R += reward\n",
    "        steps += 1\n",
    "\n",
    "    print(R, steps)\n",
    "# -559.8001999999993 1691"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "96\n",
      "-180.32318 269\n",
      "0\n",
      "-509.42016999999987 1663\n",
      "2\n",
      "-470.51302000000027 1689\n",
      "80\n",
      "-329.1735099999998 709\n",
      "0\n",
      "-387.2399700000006 1613\n",
      "0\n",
      "-441.77567999999997 1693\n",
      "0\n",
      "-518.9046599999998 1691\n",
      "0\n",
      "-524.7712200000009 1900\n",
      "0\n",
      "-453.4716200000006 1703\n",
      "0\n",
      "-506.7430599999998 1775\n",
      "1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_72903/1786941437.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" source exec : {} - chosen : {} \\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_exec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_72903/3197634596.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, early_stop)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrontier_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_exec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/references/decima/spark_env/env.py\u001b[0m in \u001b[0;36mnew_step\u001b[0;34m(self, next_node, limit)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_dags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwall_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_time\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_time\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m                    \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_dags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "with open('temp.txt', 'a') as fp:\n",
    "    for node in nodes:\n",
    "        fp.write(node)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "len(nodes)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(R, steps, nodes[1619])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-479.6533599999999 1619 2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, action_space=100, num_inputs=501):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        # mu.weight.data.mul_(0.1)\n",
    "        # mu.bias.data.mul_(0.1)\n",
    "\n",
    "        self.l1    = nn.Linear(num_inputs, 128)\n",
    "        torch.nn.init.xavier_normal_(self.l1.weight)\n",
    "        self.ln1   = nn.LayerNorm(128)\n",
    "\n",
    "        self.l2    = nn.Linear(128, 128)\n",
    "        torch.nn.init.xavier_normal_(self.l2.weight)\n",
    "        self.ln2   = nn.LayerNorm(128)\n",
    "\n",
    "        self.l3    = nn.Linear(128, 64)\n",
    "        torch.nn.init.xavier_normal_(self.l3.weight)\n",
    "        self.ln3   = nn.LayerNorm(64)\n",
    "            \n",
    "        self.l4    = nn.Linear(64, action_space)\n",
    "        torch.nn.init.xavier_normal_(self.l4.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.l1(inputs)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.l2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.l3(x)\n",
    "        x = self.ln3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.l4(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self,  action_space=200, num_inputs=501):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l1    = nn.Linear(num_inputs, 128)\n",
    "        torch.nn.init.xavier_normal_(self.l1.weight)\n",
    "        self.ln1   = nn.LayerNorm(128)\n",
    "\n",
    "        self.l2    = nn.Linear(128+action_space, 128)\n",
    "        torch.nn.init.xavier_normal_(self.l2.weight)\n",
    "        self.ln2   = nn.LayerNorm(128)\n",
    "\n",
    "        self.l3    = nn.Linear(128, 64)\n",
    "        torch.nn.init.xavier_normal_(self.l3.weight)\n",
    "        self.ln3   = nn.LayerNorm(64)\n",
    "\n",
    "        self.V = nn.Linear(64, 1)\n",
    "        torch.nn.init.xavier_normal_(self.V.weight)\n",
    "        # self.V.weight.data.mul_(0.1)\n",
    "        # self.V.bias.data.mul_(0.1)\n",
    "\n",
    "    def forward(self, inputs, actions):\n",
    "        \n",
    "        x = self.l1(inputs)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = torch.cat((x, actions))\n",
    "        x = self.l2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.l3(x)\n",
    "        x = self.ln3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        V = self.V(x)\n",
    "        \n",
    "        return V"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, action_space=100, num_inputs=501):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor_node        = Actor(action_space=action_space, num_inputs=num_inputs)\n",
    "        self.actor_parallelism = Actor(action_space=action_space, num_inputs=num_inputs)\n",
    "        self.critic            = Critic(action_space=action_space*2, num_inputs=num_inputs)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        action_node        = self.actor_node(state)\n",
    "        action_parallelism = self.actor_parallelism(state)\n",
    "        value_pred         = self.critic(state, torch.cat([action_node, action_parallelism]))\n",
    "        \n",
    "        return action_node, action_parallelism, value_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, gamma=0.9995) -> None:\n",
    "        super().__init__()\n",
    "        self.model = ActorCritic().to(cuda)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        self.file = \"actor_critic\"\n",
    "\n",
    "    def __init_weights__(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        step_reward = 0\n",
    "        policy_node = [] # list to save actor (policy) loss\n",
    "        policy_parallelism = []\n",
    "        value_losses = [] # list to save critic (value) loss\n",
    "        returns = [] # list to save the true values\n",
    "\n",
    "        # calculate the true value using rewards returned from the environment\n",
    "        for reward in self.rewards:\n",
    "            # calculate the discounted value\n",
    "            step_reward = reward + self.gamma * step_reward\n",
    "            returns.insert(0, step_reward)\n",
    "\n",
    "        returns = torch.tensor(returns).to(cuda)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
    "\n",
    "        for (log_prob_1, log_prob_2, value), reward in zip(self.saved_actions, returns):\n",
    "            advantage = reward - value.item()\n",
    "\n",
    "            # calculate actor (policy) loss \n",
    "            policy_node.append(-log_prob_1 * advantage)\n",
    "            policy_parallelism.append(-log_prob_2 * advantage)\n",
    "\n",
    "            # calculate critic (value) loss using L1 smooth loss\n",
    "            value_losses.append(F.smooth_l1_loss(value, torch.tensor([reward]).to(cuda)))\n",
    "\n",
    "        # reset gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # sum up all the values of policy_losses and value_losses\n",
    "        loss_node   = torch.stack(policy_node).to(cuda).sum()\n",
    "        loss_parallelism = torch.stack(policy_parallelism).to(cuda).sum()\n",
    "        loss_critic = torch.stack(value_losses).to(cuda).sum()\n",
    "\n",
    "        # perform backprop\n",
    "        loss = loss_node + loss_parallelism + loss_critic\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "\n",
    "        # reset rewards and action buffer\n",
    "        self.saved_actions.clear()\n",
    "        self.rewards.clear()\n",
    "        \n",
    "    def act(self, state):\n",
    "        probs_node, probs_parallelism, value = self.model(state)\n",
    "\n",
    "        # create a categorical distribution over the list of probabilities of actions\n",
    "        acts_node = torch.distributions.Categorical(logits=probs_node)                      \n",
    "        acts_parallelism = torch.distributions.Categorical(logits=probs_parallelism) \n",
    "\n",
    "        # and sample an action using the distribution\n",
    "        action_node = acts_node.sample()\n",
    "        action_parallelism = acts_parallelism.sample()\n",
    "\n",
    "        # save to action buffer\n",
    "        self.saved_actions.append((acts_node.log_prob(action_node), acts_parallelism.log_prob(action_parallelism), value))\n",
    "\n",
    "        # the action to take (node, dir) and (parallelism)\n",
    "        return action_node.item(), action_parallelism.item()\n",
    "\n",
    "    def save(self, version=\"1\"):\n",
    "        torch.save(self.model.state_dict(), self.file+\"_\"+version+\".pt\")\n",
    "\n",
    "    def load(self, file=\"./actor_critic\", version=\"1\"):\n",
    "        self.file = file\n",
    "        self.model.load_state_dict(torch.load(file+\"_\"+version+\".pt\"))\n",
    "        self.model.eval()\n",
    "\n",
    "    def direct_train(self, env:EnvironmentWrapper, episodes=70):\n",
    "\n",
    "        node_pred = []\n",
    "        parallelism_pred = []\n",
    "\n",
    "        node_act = []\n",
    "        parallelism_act = []\n",
    "        value_act = []\n",
    "\n",
    "        running_reward = 10\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            env.reset()\n",
    "            state = env.observe()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                # get prediction\n",
    "                node, parallelism, value = self.model(state)\n",
    "\n",
    "                # get actions from logits for nodes and paralellism limit\n",
    "                node_pred.append(node); parallelism_pred.append(parallelism)\n",
    "\n",
    "                # use random actions within bounds to train for action\n",
    "                n_a = randint(0, 100); p_a = randint(2, 5)\n",
    "                n = np.zeros(100); p = np.zeros(100); n[n_a] = 1; p[p_a] = 1\n",
    "                \n",
    "                n_t = torch.from_numpy(n).type(torch.FloatTensor)\n",
    "                p_t = torch.from_numpy(p).type(torch.FloatTensor)\n",
    "                node_act.append(n_t)\n",
    "                parallelism_act.append(p_t)\n",
    "\n",
    "                # predic value from critic for random actions\n",
    "                value = self.model.critic( state, torch.cat([n_t.to(cuda), p_t.to(cuda)]) )\n",
    "                \n",
    "                # get step and compute actual reward value for random actions\n",
    "                state, reward, done = env.step(torch.tensor([int(n_a/50), n_a%50, p_a]))\n",
    "                value_act.append(F.smooth_l1_loss(value, torch.tensor([reward]).type(torch.FloatTensor).to(cuda)))\n",
    "\n",
    "                episode_reward += reward\n",
    "            \n",
    "            n_l = nn.MSELoss()\n",
    "            p_l = nn.MSELoss()\n",
    "            loss_node        = n_l(torch.stack(node_pred), torch.stack(node_act).to(cuda))\n",
    "            loss_parallelism = p_l(torch.stack(parallelism_pred), torch.stack(parallelism_act).to(cuda))\n",
    "            loss_critic      = torch.stack(value_act).to(cuda).sum()\n",
    "\n",
    "            loss_node.backward(retain_graph=True)\n",
    "            loss_parallelism.backward(retain_graph=True)\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            self.save()\n",
    "\n",
    "            running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "            # log results\n",
    "            if i % 5 == 0:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                    i, episode_reward, running_reward))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# agent = Agent()\n",
    "# agent.direct_train(env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import time\n",
    "\n",
    "episodes = 10\n",
    "running_reward = 0\n",
    "\n",
    "agent = Agent()\n",
    "agent.load(file=\"./actor_critic\", version=\"3\")\n",
    "\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    env.reset()\n",
    "    state = env.observe()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    start_time = time.time()\n",
    "    actions = 0\n",
    "    while not done:\n",
    "        node, parallelism = agent.act(state)\n",
    "        direc = int(node/50)\n",
    "        node  = node % 50\n",
    "        state, reward, done = env.step((direc, node, parallelism))\n",
    "        agent.rewards.append(reward)\n",
    "        episode_reward += reward\n",
    "        actions += 1\n",
    "\n",
    "    # print(actions, time.time()-start_time)\n",
    "    # agent.learn()\n",
    "    # agent.save(version=\"3\")\n",
    "    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "    # log results\n",
    "    if e % 5 == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "            e, episode_reward, running_reward))\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 0\tLast reward: -549.49\tAverage reward: -27.47\n",
      "Episode 5\tLast reward: 1.00\tAverage reward: -21.03\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test for the trained network\n",
    "\n",
    "ep = 10\n",
    "for e in range(10):\n",
    "\n",
    "    env.reset()\n",
    "    state = env.observe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# from matplotlib import pylab\n",
    "\n",
    "# def save_graph(graph, file_name):\n",
    "#     plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "#     plt.axis('off')\n",
    "#     fig = plt.figure(1)\n",
    "#     pos = nx.spring_layout(graph)\n",
    "#     nx.draw_networkx_nodes(graph,pos)\n",
    "#     nx.draw_networkx_edges(graph,pos)\n",
    "#     nx.draw_networkx_labels(graph,pos)\n",
    "\n",
    "#     plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "#     pylab.close()\n",
    "#     del fig\n",
    "\n",
    "# save_graph(job_graph, \"job_dag\")\n",
    "\n",
    "\n",
    "# DDQN implementation,\n",
    "\n",
    "# import copy\n",
    "# from collections import deque\n",
    "# import random\n",
    "# import time, datetime\n",
    "\n",
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim=501, output_dim=3):\n",
    "#         super().__init__()\n",
    "#         self.static = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 501),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(501, 250),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(250, 200),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(200, 150),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(150, output_dim)\n",
    "#         )\n",
    "\n",
    "#         self.dynamic = copy.deepcopy(self.static)\n",
    "\n",
    "#         for p in self.dynamic.parameters():\n",
    "#             p.requires_grad = False\n",
    "\n",
    "#     def forward(self, input, model=\"static\"):\n",
    "#         if model == \"static\":\n",
    "#             return self.static(input)\n",
    "            \n",
    "#         return self.dynamic(input)\n",
    "\n",
    "# class Agent():\n",
    "\n",
    "#     def __init__(self, state_dim=501, action_dim=(2, 50, 100), save_dir=\".\", assist=False, assist_p=(2, 7)):\n",
    "#         self.state_dim = state_dim\n",
    "#         self.action_dim = action_dim\n",
    "#         self.save_dir = save_dir\n",
    "\n",
    "#         self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "#         # DNN to predict the most optimal action\n",
    "#         self.net = Net(self.state_dim, len(self.action_dim)).float()\n",
    "#         self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "#         self.exploration_rate = 1\n",
    "#         self.exploration_rate_decay = 0.99999975\n",
    "#         self.exploration_rate_min = 0.1\n",
    "#         self.curr_step = 0\n",
    "\n",
    "#         self.memory = deque(maxlen=100000)\n",
    "#         self.batch_size = 32\n",
    "\n",
    "#         self.save_every = 5e3  # no. of experiences\n",
    "\n",
    "#         self.gamma = 0.9\n",
    "\n",
    "#         self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "#         self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#         self.burnin = 1e4  # min. experiences before training\n",
    "#         self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "#         self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "#         self.assist = assist\n",
    "#         self.assist_range = assist_p\n",
    "\n",
    "#     def act(self, state):\n",
    "#         # EXPLORE\n",
    "#         if np.random.rand() < self.exploration_rate:\n",
    "#             direction, job, executor = self.action_dim\n",
    "#             if self.assist:\n",
    "#                 action_idx = (np.random.randint(direction), np.random.randint(job)/job, np.random.randint(self.assist_range[0], self.assist_range[1])/executor)\n",
    "#             else :\n",
    "#                 action_idx = torch.tensor([np.random.randint(direction), np.random.randint(job)/job, np.random.randint(1, executor)/executor])\n",
    "\n",
    "#         # EXPLOIT\n",
    "#         else:\n",
    "#             state = state.cuda()\n",
    "#             action_idx = self.net(state, model=\"dynamic\")\n",
    "\n",
    "#         # decrease exploration_rate\n",
    "#         self.exploration_rate *= self.exploration_rate_decay\n",
    "#         self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "#         # increment step\n",
    "#         self.curr_step += 1\n",
    "#         return action_idx\n",
    "\n",
    "#     def cache(self, state, next_state, action, reward, done):\n",
    "#         reward = torch.tensor([reward]).cuda()\n",
    "#         done = torch.tensor([done]).cuda()\n",
    "#         state = state.cuda()\n",
    "#         next_state = next_state.cuda()\n",
    "#         action = action.cuda()\n",
    "#         self.memory.append((state, next_state, action, reward, done))\n",
    "\n",
    "#     def recall(self):\n",
    "#         batch = random.sample(self.memory, self.batch_size)\n",
    "#         state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "#         return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "#     def td_estimate(self, state, action):\n",
    "#         current_Q = self.net(state, model=\"static\")[\n",
    "#             np.arange(0, self.batch_size), action\n",
    "#         ]  # Q_online(s,a)\n",
    "#         return current_Q\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def td_target(self, reward, next_state, done):\n",
    "#         best_action = self.net(next_state, model=\"static\")\n",
    "#         next_Q = self.net(next_state, model=\"dynamic\")[\n",
    "#             np.arange(0, self.batch_size, dtype=np.int64), best_action\n",
    "#         ]\n",
    "#         return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "#     def update_Q_online(self, td_estimate, td_target):\n",
    "#         loss = self.loss_fn(td_estimate, td_target)\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def sync_Q_target(self):\n",
    "#         self.net.dynamic.load_state_dict(self.net.dynamic.state_dict())\n",
    "\n",
    "#     def save(self):\n",
    "#         save_path = (\n",
    "#             self.save_dir + f\"/sched_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "#         )\n",
    "#         torch.save(\n",
    "#             dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "#             save_path,\n",
    "#         )\n",
    "#         print(f\"Sched_net saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "#     def learn(self):\n",
    "#         if self.curr_step % self.sync_every == 0:\n",
    "#             self.sync_Q_target()\n",
    "\n",
    "#         if self.curr_step % self.save_every == 0:\n",
    "#             self.save()\n",
    "\n",
    "#         if self.curr_step < self.burnin:\n",
    "#             return None, None\n",
    "\n",
    "#         if self.curr_step % self.learn_every != 0:\n",
    "#             return None, None\n",
    "\n",
    "#         # Sample from memory\n",
    "#         state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "#         # Get TD Estimate\n",
    "#         td_est = self.td_estimate(state, action)\n",
    "\n",
    "#         # Get TD Target\n",
    "#         td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "#         # Backpropagate loss through Q_online\n",
    "#         loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "#         return (td_est.mean().item(), loss)\n",
    "\n",
    "# class MetricLogger:\n",
    "#     def __init__(self, save_dir:str):\n",
    "#         self.save_log = save_dir + \"/log\"\n",
    "#         with open(self.save_log, \"w\") as f:\n",
    "#             f.write(\n",
    "#                 f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "#                 f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "#                 f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "#             )\n",
    "#         self.ep_rewards_plot = save_dir + \"/reward_plot.jpg\"\n",
    "#         self.ep_lengths_plot = save_dir + \"/length_plot.jpg\"\n",
    "#         self.ep_avg_losses_plot = save_dir + \"/loss_plot.jpg\"\n",
    "#         self.ep_avg_qs_plot = save_dir + \"/q_plot.jpg\"\n",
    "\n",
    "#         # History metrics\n",
    "#         self.ep_rewards = []\n",
    "#         self.ep_lengths = []\n",
    "#         self.ep_avg_losses = []\n",
    "#         self.ep_avg_qs = []\n",
    "\n",
    "#         # Moving averages, added for every call to record()\n",
    "#         self.moving_avg_ep_rewards = []\n",
    "#         self.moving_avg_ep_lengths = []\n",
    "#         self.moving_avg_ep_avg_losses = []\n",
    "#         self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "#         # Current episode metric\n",
    "#         self.init_episode()\n",
    "\n",
    "#         # Timing\n",
    "#         self.record_time = time.time()\n",
    "\n",
    "#     def log_step(self, reward, loss, q):\n",
    "#         self.curr_ep_reward += reward\n",
    "#         self.curr_ep_length += 1\n",
    "#         if loss:\n",
    "#             self.curr_ep_loss += loss\n",
    "#             self.curr_ep_q += q\n",
    "#             self.curr_ep_loss_length += 1\n",
    "\n",
    "#     def log_episode(self):\n",
    "#         \"Mark end of episode\"\n",
    "#         self.ep_rewards.append(self.curr_ep_reward)\n",
    "#         self.ep_lengths.append(self.curr_ep_length)\n",
    "#         if self.curr_ep_loss_length == 0:\n",
    "#             ep_avg_loss = 0\n",
    "#             ep_avg_q = 0\n",
    "#         else:\n",
    "#             ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "#             ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "#         self.ep_avg_losses.append(ep_avg_loss)\n",
    "#         self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "#         self.init_episode()\n",
    "\n",
    "#     def init_episode(self):\n",
    "#         self.curr_ep_reward = 0.0\n",
    "#         self.curr_ep_length = 0\n",
    "#         self.curr_ep_loss = 0.0\n",
    "#         self.curr_ep_q = 0.0\n",
    "#         self.curr_ep_loss_length = 0\n",
    "\n",
    "#     def record(self, episode, epsilon, step):\n",
    "#         mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "#         mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "#         mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "#         mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "#         self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "#         self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "#         self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "#         self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "#         last_record_time = self.record_time\n",
    "#         self.record_time = time.time()\n",
    "#         time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "#         print(\n",
    "#             f\"Episode {episode} - \"\n",
    "#             f\"Step {step} - \"\n",
    "#             f\"Epsilon {epsilon} - \"\n",
    "#             f\"Mean Reward {mean_ep_reward} - \"\n",
    "#             f\"Mean Length {mean_ep_length} - \"\n",
    "#             f\"Mean Loss {mean_ep_loss} - \"\n",
    "#             f\"Mean Q Value {mean_ep_q} - \"\n",
    "#             f\"Time Delta {time_since_last_record} - \"\n",
    "#             f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "#         )\n",
    "\n",
    "#         with open(self.save_log, \"a\") as f:\n",
    "#             f.write(\n",
    "#                 f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "#                 f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "#                 f\"{time_since_last_record:15.3f}\"\n",
    "#                 f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "#             )\n",
    "\n",
    "#         for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "#             plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "#             plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "#             plt.clf()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}