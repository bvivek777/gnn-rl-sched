{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "from spark_env.env import Environment\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import dgl\n",
    "import torch\n",
    "from spark_env.job_dag import JobDAG\n",
    "from spark_env.node import Node\n",
    "import torch.nn as nn\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, features=5, hidden_layer_size=10, embedding_size=10, layers=3):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv = []\n",
    "        self.layers = layers\n",
    "        self.conv.append(GraphConv(features, hidden_layer_size))\n",
    "        for i in range(layers-2):\n",
    "            self.conv.append(GraphConv(hidden_layer_size, hidden_layer_size))\n",
    "        self.conv.append(GraphConv(hidden_layer_size, embedding_size))\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        for i in range(self.layers):\n",
    "            h = self.conv[i](g, h)\n",
    "            if i != self.layers-1:\n",
    "                h = torch.relu(h)\n",
    "        return h\n",
    "\n",
    "def translate_state(obs, node_input_dim=5, job_input_dim=3):\n",
    "        \"\"\"\n",
    "        Translate the observation to matrix form\n",
    "        \"\"\"\n",
    "        job_dags, source_job, num_source_exec, frontier_nodes, executor_limits, \\\n",
    "        exec_commit, moving_executors, action_map = obs\n",
    "\n",
    "        # compute total number of nodes\n",
    "        total_num_nodes = int(np.sum([job_dag.num_nodes for job_dag in job_dags]))\n",
    "\n",
    "        # job and node inputs to feed\n",
    "        node_inputs = np.zeros([total_num_nodes, node_input_dim])\n",
    "        job_inputs = np.zeros([len(job_dags), job_input_dim])\n",
    "\n",
    "        # sort out the exec_map\n",
    "        exec_map = {}\n",
    "        for job_dag in job_dags:\n",
    "            exec_map[job_dag] = len(job_dag.executors)\n",
    "        # count in moving executors\n",
    "        for node in moving_executors.moving_executors.values():\n",
    "            exec_map[node.job_dag] += 1\n",
    "        # count in executor commit\n",
    "        for s in exec_commit.commit:\n",
    "            if isinstance(s, JobDAG):\n",
    "                j = s\n",
    "            elif isinstance(s, Node):\n",
    "                j = s.job_dag\n",
    "            elif s is None:\n",
    "                j = None\n",
    "            else:\n",
    "                print('source', s, 'unknown')\n",
    "                exit(1)\n",
    "            for n in exec_commit.commit[s]:\n",
    "                if n is not None and n.job_dag != j:\n",
    "                    exec_map[n.job_dag] += exec_commit.commit[s][n]\n",
    "\n",
    "        # gather job level inputs\n",
    "        job_idx = 0\n",
    "        for job_dag in job_dags:\n",
    "            # number of executors in the job\n",
    "            job_inputs[job_idx, 0] = exec_map[job_dag] / 20.0\n",
    "            # the current executor belongs to this job or not\n",
    "            if job_dag is source_job:\n",
    "                job_inputs[job_idx, 1] = 2\n",
    "            else:\n",
    "                job_inputs[job_idx, 1] = -2\n",
    "            # number of source executors\n",
    "            job_inputs[job_idx, 2] = num_source_exec / 20.0\n",
    "\n",
    "            job_idx += 1\n",
    "\n",
    "        # gather node level inputs\n",
    "        node_idx = 0\n",
    "        job_idx = 0\n",
    "        for job_dag in job_dags:\n",
    "            for node in job_dag.nodes:\n",
    "\n",
    "                # copy the feature from job_input first\n",
    "                node_inputs[node_idx, :3] = job_inputs[job_idx, :3]\n",
    "\n",
    "                # work on the node\n",
    "                node_inputs[node_idx, 3] = \\\n",
    "                    (node.num_tasks - node.next_task_idx) * \\\n",
    "                    node.tasks[-1].duration / 100000.0\n",
    "\n",
    "                # number of tasks left\n",
    "                node_inputs[node_idx, 4] = \\\n",
    "                    (node.num_tasks - node.next_task_idx) / 200.0\n",
    "\n",
    "                node_idx += 1\n",
    "\n",
    "            job_idx += 1\n",
    "\n",
    "        return node_inputs\n",
    "\n",
    "\n",
    "net = GCN()\n",
    "env_net = GCN(features=10, hidden_layer_size=100, embedding_size=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "env = Environment()\n",
    "done = False\n",
    "reset_prob = 5e-7\n",
    "env.seed(234)\n",
    "env.reset(max_time=np.random.geometric(reset_prob))\n",
    "# nx.draw(job_graph, with_labels=True)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# get the first obervation\n",
    "job_graph, frontier_nodes, executor_limits, exec_graph, action_map = env.new_observation()\n",
    "node_inputs = translate_state(env.observe())\n",
    "\n",
    "# convert the observation to required format\n",
    "dgl_graph = dgl.from_networkx(job_graph)\n",
    "node_inputs = torch.tensor(node_inputs)\n",
    "G = dgl.add_self_loop(dgl_graph)\n",
    "inputs = node_inputs.type(torch.FloatTensor)\n",
    "\n",
    "# compute the logits for the graph\n",
    "logits = net(G, inputs)\n",
    "\n",
    "# extract only the required indices (only the frontier nodes)\n",
    "# and generate a new summary graph\n",
    "required_indices = []\n",
    "summary_graph = nx.DiGraph()\n",
    "schedulable_nodes = len(frontier_nodes)\n",
    "summary_graph.add_node(schedulable_nodes)\n",
    "\n",
    "# loop over the frontier nodes seperating the features as well as \n",
    "# adding nodes to the new graph\n",
    "for node in frontier_nodes:\n",
    "    vertex = action_map.inverse_map[node]\n",
    "    summary_graph.add_node(vertex)\n",
    "    summary_graph.add_edge(vertex, schedulable_nodes)\n",
    "    required_indices.append(action_map.inverse_map[node])\n",
    "\n",
    "# calculate features for new graph and convert the input to a required format\n",
    "e_input = logits[required_indices]\n",
    "e_input = torch.cat((e_input, torch.ones(1, 10)), 0)\n",
    "G_s = dgl.from_networkx(summary_graph)\n",
    "G_s = dgl.add_self_loop(G_s)\n",
    "\n",
    "# convert the summary graph to logits\n",
    "logits_s = env_net(G_s, e_input)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Net():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import deque\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, state_dim=100, action_dim=2, save_dir=\".\"):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = Net(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "# obs = env.observe()\n",
    "# job_dags, source_job, num_source_exec, frontier_nodes, executor_limits, exec_commit, moving_executors, action_map = obs\n",
    "# total_reward = 0\n",
    "# loops = 0\n",
    "\n",
    "# dag_list = []\n",
    "# dag_map = {}\n",
    "\n",
    "# while not done:\n",
    "    \n",
    "#     eq = np.random.randint(1, 5)\n",
    "#     eq = min(eq, num_source_exec)\n",
    "\n",
    "#     if(len(frontier_nodes)) == 0:\n",
    "#         break\n",
    "\n",
    "#     for node in frontier_nodes:\n",
    "\n",
    "#         if len(dag_list) == 0:\n",
    "#             dag_map[node.job_dag] = True\n",
    "#             dag_list.append((node, action_map.inverse_map[node]))\n",
    "#         elif node.job_dag in dag_map:\n",
    "#             dag_list.append((node, action_map.inverse_map[node]))\n",
    "\n",
    "#         obs, reward, done = env.step(node, eq)\n",
    "#         break\n",
    "\n",
    "#     total_reward += reward\n",
    "#     if not done:\n",
    "#         job_dags, source_job, num_source_exec, frontier_nodes, executor_limits, exec_commit, moving_executors, action_map = obs\n",
    "#         print(moving_executors)\n",
    "#     input()\n",
    "\n",
    "# print(total_reward)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "# from matplotlib import pylab\n",
    "\n",
    "# def save_graph(graph, file_name):\n",
    "#     plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "#     plt.axis('off')\n",
    "#     fig = plt.figure(1)\n",
    "#     pos = nx.spring_layout(graph)\n",
    "#     nx.draw_networkx_nodes(graph,pos)\n",
    "#     nx.draw_networkx_edges(graph,pos)\n",
    "#     nx.draw_networkx_labels(graph,pos)\n",
    "\n",
    "#     plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "#     pylab.close()\n",
    "#     del fig\n",
    "\n",
    "# save_graph(job_graph, \"job_dag\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}