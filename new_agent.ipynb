{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from spark_env.env import Environment\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import dgl\n",
    "import torch\n",
    "from spark_env.job_dag import JobDAG\n",
    "from spark_env.node import Node\n",
    "import torch.nn as nn\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, features=5, hidden_layer_size=10, embedding_size=10, layers=3):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv = []\n",
    "        self.layers = layers\n",
    "        self.conv.append(GraphConv(features, hidden_layer_size))\n",
    "        for i in range(layers-2):\n",
    "            self.conv.append(GraphConv(hidden_layer_size, hidden_layer_size))\n",
    "        self.conv.append(GraphConv(hidden_layer_size, embedding_size))\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        for i in range(self.layers):\n",
    "            h = self.conv[i](g, h)\n",
    "            if i != self.layers-1:\n",
    "                h = torch.relu(h)\n",
    "        return h\n",
    "\n",
    "class EnvironmentWrapper:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.env = Environment()\n",
    "        self.offset = 0\n",
    "        self.reset_prob = 5e-7\n",
    "        self.net = GCN()\n",
    "        self.max_exec = 100\n",
    "        self.range = 50\n",
    "        self.reset()\n",
    "        self.frontier_nodes = []\n",
    "        self.action_map = {}\n",
    "        self.required_indices = []\n",
    "\n",
    "    def translate_state(self, job_dags, source_job, num_source_exec, exec_commit, moving_executors, node_input_dim=5, job_input_dim=3):\n",
    "        \"\"\"\n",
    "        Translate the observation to matrix form\n",
    "        \"\"\"\n",
    "\n",
    "        # compute total number of nodes\n",
    "        total_num_nodes = int(np.sum([job_dag.num_nodes for job_dag in job_dags]))\n",
    "\n",
    "        # job and node inputs to feed\n",
    "        node_inputs = np.zeros([total_num_nodes, node_input_dim])\n",
    "        job_inputs = np.zeros([len(job_dags), job_input_dim])\n",
    "\n",
    "        # sort out the exec_map\n",
    "        exec_map = {}\n",
    "        for job_dag in job_dags:\n",
    "            exec_map[job_dag] = len(job_dag.executors)\n",
    "        # count in moving executors\n",
    "        for node in moving_executors.moving_executors.values():\n",
    "            exec_map[node.job_dag] += 1\n",
    "        # count in executor commit\n",
    "        for s in exec_commit.commit:\n",
    "            if isinstance(s, JobDAG):\n",
    "                j = s\n",
    "            elif isinstance(s, Node):\n",
    "                j = s.job_dag\n",
    "            elif s is None:\n",
    "                j = None\n",
    "            else:\n",
    "                print('source', s, 'unknown')\n",
    "                exit(1)\n",
    "            for n in exec_commit.commit[s]:\n",
    "                if n is not None and n.job_dag != j:\n",
    "                    exec_map[n.job_dag] += exec_commit.commit[s][n]\n",
    "\n",
    "        # gather job level inputs\n",
    "        job_idx = 0\n",
    "        for job_dag in job_dags:\n",
    "            # number of executors in the job\n",
    "            job_inputs[job_idx, 0] = exec_map[job_dag] / 20.0\n",
    "            # the current executor belongs to this job or not\n",
    "            if job_dag is source_job:\n",
    "                job_inputs[job_idx, 1] = 2\n",
    "            else:\n",
    "                job_inputs[job_idx, 1] = -2\n",
    "            # number of source executors\n",
    "            job_inputs[job_idx, 2] = num_source_exec / 20.0\n",
    "\n",
    "            job_idx += 1\n",
    "\n",
    "        # gather node level inputs\n",
    "        node_idx = 0\n",
    "        job_idx = 0\n",
    "        for job_dag in job_dags:\n",
    "            for node in job_dag.nodes:\n",
    "\n",
    "                # copy the feature from job_input first\n",
    "                node_inputs[node_idx, :3] = job_inputs[job_idx, :3]\n",
    "\n",
    "                # work on the node\n",
    "                node_inputs[node_idx, 3] = \\\n",
    "                    (node.num_tasks - node.next_task_idx) * \\\n",
    "                    node.tasks[-1].duration / 100000.0\n",
    "\n",
    "                # number of tasks left\n",
    "                node_inputs[node_idx, 4] = \\\n",
    "                    (node.num_tasks - node.next_task_idx) / 200.0\n",
    "\n",
    "                node_idx += 1\n",
    "\n",
    "            job_idx += 1\n",
    "\n",
    "        return node_inputs\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.seed(np.random.randint(12, 512))\n",
    "        self.env.reset(max_time=np.random.geometric(self.reset_prob))\n",
    "        self.offset = 0\n",
    "\n",
    "    def observe(self):\n",
    "        job_graph, frontier_nodes, executor_limits, action_map, \\\n",
    "            job_dags, source_job, num_source_exec, \\\n",
    "               exec_commit, moving_executors = self.env.new_observation()\n",
    "\n",
    "        if len(frontier_nodes) == 0:\n",
    "            return torch.zeros(501)\n",
    "        \n",
    "        node_inputs = self.translate_state(job_dags, source_job, num_source_exec, exec_commit, moving_executors)\n",
    "        logits = self.get_graph_embedding(job_graph, node_inputs, frontier_nodes, action_map)\n",
    "        logits = logits.flatten()\n",
    "        executor_limits /= self.max_exec\n",
    "        if len(logits) < 500:\n",
    "            logits = torch.cat([logits, torch.zeros(500-len(logits))])\n",
    "        return torch.cat([logits, torch.tensor([executor_limits])])\n",
    "\n",
    "    def step(self, action):\n",
    "        direction, job, limit = action \n",
    "        direction = direction.item()\n",
    "        job = job.item()\n",
    "        limit = limit.item()\n",
    "        \n",
    "\n",
    "    def get_graph_embedding(self, job_graph, node_inputs, frontier_nodes, action_map):\n",
    "        # convert the observation to required format\n",
    "        dgl_graph = dgl.from_networkx(job_graph)\n",
    "        node_inputs = torch.tensor(node_inputs)\n",
    "        G = dgl.add_self_loop(dgl_graph)\n",
    "        inputs = node_inputs.type(torch.FloatTensor)\n",
    "\n",
    "        # compute the logits for the graph\n",
    "        logits = self.net(G, inputs)\n",
    "\n",
    "        # extract only the required indices (only the frontier nodes)\n",
    "        # and generate a new summary graph\n",
    "        frontier_indices = []\n",
    "        self.frontier_nodes = []\n",
    "\n",
    "        # loop over the frontier nodes seperating the features as well as \n",
    "        # adding nodes to the new environment\n",
    "        for node in frontier_nodes:\n",
    "            frontier_indices.append(action_map.inverse_map[node])\n",
    "\n",
    "        required_indices = []\n",
    "        for i in range(1, self.range):\n",
    "            index = (self.offset+i)%len(frontier_indices)\n",
    "            self.frontier_nodes.append(action_map.map[index])\n",
    "            required_indices.append(frontier_indices[index])\n",
    "            if len(required_indices) == len(frontier_indices):\n",
    "                break\n",
    "\n",
    "        # calculate features for new graph and convert the input to a required format\n",
    "        logits = logits[required_indices]\n",
    "\n",
    "        return logits\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "import copy\n",
    "from collections import deque\n",
    "import random\n",
    "import time, datetime\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=500, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.static = nn.Sequential(\n",
    "            nn.Linear(input_dim, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, output_dim)\n",
    "        )\n",
    "\n",
    "        self.dynamic = copy.deepcopy(self.online)\n",
    "\n",
    "    def forward(self, input, model=\"static\"):\n",
    "        if model == \"static\":\n",
    "            return self.static(input)\n",
    "            \n",
    "        return self.dynamic(input)\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, state_dim=500, action_dim=(3, 50, 100), save_dir=\".\"):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # DNN to predict the most optimal action\n",
    "        self.net = Net(self.state_dim, len(self.action_dim)).float()\n",
    "        self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.save_every = 5e3  # no. of experiences\n",
    "\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def act(self, state):\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            direction, job, executor = self.action_dim\n",
    "            action_idx = (np.random.randint(direction), np.random.random(job), np.random.random(executor))\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"dynamic\")\n",
    "            action_idx = (action_values * 10**3).round() / (10**3)\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        state = torch.tensor(state).cuda()\n",
    "        next_state = torch.tensor(next_state).cuda()\n",
    "        action = torch.tensor([action]).cuda()\n",
    "        reward = torch.tensor([reward]).cuda()\n",
    "        done = torch.tensor([done]).cuda()\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"static\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"static\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"dynamic\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"sched_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"Sched_net saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "env = Environment()   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "agent = Agent(state_dim=500, action_dim=3, save_dir=\".\")\n",
    "\n",
    "logger = MetricLogger(\".\")\n",
    "\n",
    "episodes = 10\n",
    "for e in range(episodes):\n",
    "\n",
    "    \n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = agent.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode=e, epsilon=agent.exploration_rate, step=agent.curr_step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# obs = env.observe()\n",
    "# job_dags, source_job, num_source_exec, frontier_nodes, executor_limits, exec_commit, moving_executors, action_map = obs\n",
    "# total_reward = 0\n",
    "# loops = 0\n",
    "\n",
    "# dag_list = []\n",
    "# dag_map = {}\n",
    "\n",
    "# while not done:\n",
    "    \n",
    "#     eq = np.random.randint(1, 5)\n",
    "#     eq = min(eq, num_source_exec)\n",
    "\n",
    "#     if(len(frontier_nodes)) == 0:\n",
    "#         break\n",
    "\n",
    "#     for node in frontier_nodes:\n",
    "\n",
    "#         if len(dag_list) == 0:\n",
    "#             dag_map[node.job_dag] = True\n",
    "#             dag_list.append((node, action_map.inverse_map[node]))\n",
    "#         elif node.job_dag in dag_map:\n",
    "#             dag_list.append((node, action_map.inverse_map[node]))\n",
    "\n",
    "#         obs, reward, done = env.step(node, eq)\n",
    "#         break\n",
    "\n",
    "#     total_reward += reward\n",
    "#     if not done:\n",
    "#         job_dags, source_job, num_source_exec, frontier_nodes, executor_limits, exec_commit, moving_executors, action_map = obs\n",
    "#         print(moving_executors)\n",
    "#     input()\n",
    "\n",
    "# print(total_reward)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "# from matplotlib import pylab\n",
    "\n",
    "# def save_graph(graph, file_name):\n",
    "#     plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "#     plt.axis('off')\n",
    "#     fig = plt.figure(1)\n",
    "#     pos = nx.spring_layout(graph)\n",
    "#     nx.draw_networkx_nodes(graph,pos)\n",
    "#     nx.draw_networkx_edges(graph,pos)\n",
    "#     nx.draw_networkx_labels(graph,pos)\n",
    "\n",
    "#     plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "#     pylab.close()\n",
    "#     del fig\n",
    "\n",
    "# save_graph(job_graph, \"job_dag\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}